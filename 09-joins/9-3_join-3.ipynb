{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "# Large Joins\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will join two large datasets.\n",
    "\n",
    "Find out what percentage of  transactions requests get response under 50 milliseconds.\n",
    "\n",
    "- **request_time** is the `timestmap` in the request row\n",
    "- **response_time** is the `timestamp` in the response row (here `ref_id` and `response_code` will not be null)\n",
    "\n",
    "Find the elapsed time between these two.\n",
    "\n",
    "## Duration\n",
    "\n",
    "30 mins\n",
    "\n",
    "## Depends on\n",
    "\n",
    "[Lab 9.1](9-1_join-1.ipynb)\n",
    "\n",
    "\n",
    "## Step-1: Verify datsets\n",
    "\n",
    "We will join transaction data with itself.\n",
    "\n",
    "- transactions data (large data).  Sample data is in `data/transactions/transactions-sample.csv`\n",
    "\n",
    "Also optionally, verify you have this data in HDFS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Start up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    import findspark\n",
    "    findspark.init()  # uses SPARK_HOME\n",
    "    print(\"Spark found in : \", findspark.find())\n",
    "\n",
    "    import pyspark\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # use a unique tmep dir for warehouse dir, so we can run multiple spark sessions in one dir\n",
    "    import tempfile\n",
    "    tmpdir = tempfile.TemporaryDirectory()\n",
    "\n",
    "    config = ( SparkConf()\n",
    "             .setAppName(\"TestApp\")\n",
    "             .setMaster(\"local[*]\")\n",
    "             .set('executor.memory', '2g')\n",
    "             .set('spark.sql.warehouse.dir', tmpdir.name)\n",
    "             .set(\"some_property\", \"some_value\") # another example\n",
    "             )\n",
    "\n",
    "    spark = SparkSession.builder.config(conf=config).getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "print('Spark UI running on port ' + spark.sparkContext.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Load data and register table\n",
    "\n",
    "We are going to provide a schema so we can process the timestamp properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : adjust the limit \n",
    "## Keep it small for experimenting, switch to 'ALL' to load the full dataset\n",
    "\n",
    "limit_rows = 1000\n",
    "# limit_rows = 'ALL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import ArrayType, IntegerType, LongType, StringType, FloatType, TimestampType, StructType, StructField\n",
    "\n",
    "\n",
    "my_schema = StructType([\n",
    "                       StructField(\"id\", StringType(), True),\n",
    "                       StructField(\"timestamp\", TimestampType(), True),\n",
    "                       StructField(\"mti\", StringType(), True),\n",
    "                       StructField(\"card_number\", StringType(), True),\n",
    "                       StructField(\"amount_customer\", FloatType(), True),\n",
    "                       StructField(\"merchant_type\", StringType(), True),\n",
    "                       StructField(\"merchant_id\", StringType(), True),\n",
    "                       StructField(\"merchant_address\", StringType(), True),\n",
    "                       StructField(\"ref_id\", StringType(), True),\n",
    "                       StructField(\"amount_merchant\", FloatType(), True),\n",
    "                       StructField(\"response_code\", StringType(), True),\n",
    "                      ])\n",
    "\n",
    "transactions = spark.read.csv(\"../data/transactions/csv\", header=True, schema=my_schema)\n",
    "transactions.limit(limit_rows).createOrReplaceTempView(\"transactions\")\n",
    "transactions.limit(limit_rows).createOrReplaceTempView(\"transactions2\")\n",
    "\n",
    "transactions.printSchema()\n",
    "\n",
    "# transactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Large x Large Join\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s = \"\"\"\n",
    "SELECT transactions.timestamp as request_time, \n",
    "transactions2.timestamp as response_time \n",
    "from transactions join transactions2 \n",
    "ON (transactions.id = transactions2.ref_id)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(s).show(10, truncate=False)\n",
    "joined = spark.sql(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: Calculate Columns\n",
    "\n",
    "We will calculate request / response timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "joined = (joined.withColumn(\"t1\", joined[\"request_time\"].cast(\"double\")))\n",
    "joined = (joined.withColumn(\"t2\", joined[\"response_time\"].cast(\"double\")))\n",
    "joined = joined.withColumn('elapsed', (joined['t2'] - joined['t1']))\n",
    "joined.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6: Run Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find records that came in under 5 seconds\n",
    "quick_response = joined.filter (\"elapsed < 5\")\n",
    "\n",
    "total_count = joined.count()\n",
    "quick_response_count = quick_response.count()\n",
    "non_quick_response_count = total_count - quick_response_count\n",
    "quick_response_percentage = quick_response_count * 100 / total_count\n",
    "\n",
    "print (\"total_count = {:,}\".format (total_count))\n",
    "print (\"quick_response_count = {:,}\".format (quick_response_count))\n",
    "print (\"non_quick_response_count = {:,}\".format (non_quick_response_count))\n",
    "print (\"quick_response_percentage = {:,.2f} %\".format (quick_response_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-7 : Run this query on Hadoop Cluster\n",
    "\n",
    "Run the same query, using data on HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-8: Discussions\n",
    "\n",
    "So in this lab, we did large X large join.  And we can see it is expensive.\n",
    "\n",
    "What are the techniques we can use to optimize the join? Please discuss with your class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
